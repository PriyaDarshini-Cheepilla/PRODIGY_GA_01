# PRODIGY_GA_01

📌 About This Project
As part of my internship, I worked on a project that involved fine-tuning GPT-2, a powerful language model developed by OpenAI, to generate text that mimics a specific writing style. The idea was to take a pre-trained model and teach it how to "speak" like the text in a custom dataset — whether that's a professional tone, creative storytelling, or any other style.

🎯 Goal
The main goal was to train GPT-2 to produce contextually accurate and stylistically consistent text based on a given prompt. This involved:

1. Preparing and cleaning a custom text dataset,

2. Fine-tuning the GPT-2 model on this data,

3. And testing the model to see how well it learned to generate relevant and meaningful outputs.

🛠️ What I Did
1. Processed the Data: Cleaned and formatted the dataset to make it suitable for training.

2. Fine-Tuned GPT-2: Used Hugging Face’s transformers library to train the model on the custom dataset.

3. Generated Outputs: Ran the model on sample prompts to generate new text and evaluated the results.

4. Documented Everything: From setup instructions to usage examples, everything is included in this repo.


And the sample output was provided in the sample_output.txt file
